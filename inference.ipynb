{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyN/uYE1tpiL/SUkeO71Udhq"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"Xy42pwRcYBB2"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Set Working Directory\n","import os\n","BASE_DIR = '/content/drive/MyDrive/speech_understanding_project'\n","MODEL_PATH = os.path.join(BASE_DIR, 'trained_decoder.pt')\n","DATA_DIR = os.path.join(BASE_DIR, 'data')\n"]},{"cell_type":"code","source":["!pip install transformers sentencepiece torch"],"metadata":{"id":"lImoNa56YL4u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from transformers import AutoTokenizer"],"metadata":{"id":"nt5jss0IYGFa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üìç Step 4: Define Tokenizers\n","text_tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\")\n","\n","class UnitTokenizer:\n","    def __init__(self, vocab_size):\n","        self.vocab_size = vocab_size\n","    def encode(self, seq): return list(map(int, seq.split()))\n","    def decode(self, ids): return \" \".join(map(str, ids))\n","\n","unit_tokenizer = UnitTokenizer(vocab_size=100)\n"],"metadata":{"id":"EZHkkGWkYPAo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Define Model Architecture\n","class SeqDecoder(nn.Module):\n","    def __init__(self, unit_vocab_size, text_vocab_size, d_model=256, nhead=4, num_layers=4):\n","        super().__init__()\n","        self.unit_emb = nn.Embedding(unit_vocab_size, d_model)\n","        self.text_emb = nn.Embedding(text_vocab_size, d_model)\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n","            num_layers=num_layers\n","        )\n","        self.decoder = nn.TransformerDecoder(\n","            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead),\n","            num_layers=num_layers\n","        )\n","        self.out_proj = nn.Linear(d_model, text_vocab_size)\n","\n","    def forward(self, src, tgt):\n","        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n","        src = self.unit_emb(src)\n","        tgt = self.text_emb(tgt)\n","        memory = self.encoder(src, src_key_padding_mask=(src == 0))\n","        output = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n","        return self.out_proj(output)\n"],"metadata":{"id":"jRuScTbpYRdr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# üìç Step 6: Load Model\n","model = SeqDecoder(unit_vocab_size=100, text_vocab_size=text_tokenizer.vocab_size).cuda()\n","model.load_state_dict(torch.load(MODEL_PATH))\n","model.eval()\n","print(\"Model Loaded\")\n"],"metadata":{"id":"v93S3TlqYVci"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 7: Inference Function\n","def translate_units(unit_sequence, max_length=50):\n","    input_ids = torch.tensor([unit_tokenizer.encode(unit_sequence)], dtype=torch.long).cuda()\n","    src = input_ids\n","\n","    # Start with decoder input as BOS token\n","    tgt_ids = torch.tensor([[text_tokenizer.bos_token_id]], dtype=torch.long).cuda()\n","\n","    for _ in range(max_length):\n","        with torch.no_grad():\n","            logits = model(src, tgt_ids)\n","            next_token_logits = logits[:, -1, :]\n","            next_token = torch.argmax(next_token_logits, dim=-1).unsqueeze(1)\n","        if next_token.item() == text_tokenizer.eos_token_id:\n","            break\n","        tgt_ids = torch.cat((tgt_ids, next_token), dim=1)\n","\n","    return text_tokenizer.decode(tgt_ids.squeeze().tolist(), skip_special_tokens=True)\n"],"metadata":{"id":"vakurMSHYZbf"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example pseudo-phoneme sequence\n","example_units = \"23 45 12 7 9 4 3 6 23 45 12 3 9 2\"\n","translated_text = translate_units(example_units)\n","print(\"Translated HRL Text:\\n\", translated_text)\n"],"metadata":{"id":"qShHVXZqYbfg"},"execution_count":null,"outputs":[]}]}