{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPruqIpxxI4jd1fmA82heyG"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"qXqc-KK5WJUG"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Set Working Directory\n","import os\n","BASE_DIR = '/content/drive/MyDrive/speech_understanding_project'\n","DATA_DIR = os.path.join(BASE_DIR, 'data')\n","os.makedirs(DATA_DIR, exist_ok=True)\n"]},{"cell_type":"code","source":["!pip install torch torchvision torchaudio transformers sentencepiece\n"],"metadata":{"id":"QorQDD0mWdot"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import numpy as np\n","import random\n","from tqdm import tqdm\n"],"metadata":{"id":"rnhaORPLWbWh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 4: Load Data\n","# Format: <utt_id>\\t<unit_seq>\n","def load_units(file_path):\n","    with open(file_path, 'r') as f:\n","        return [line.strip().split('\\t')[1] for line in f.readlines()]\n","\n","def load_text(file_path):\n","    with open(file_path, 'r') as f:\n","        return [line.strip() for line in f.readlines()]\n","\n","unit_file = os.path.join(DATA_DIR, 'units/lrl_units.txt')\n","text_file = os.path.join(DATA_DIR, 'text/hrl_text.txt')\n","\n","unit_seqs = load_units(unit_file)\n","text_seqs = load_text(text_file)\n","\n","assert len(unit_seqs) == len(text_seqs), \"Mismatch between unit and text pairs\"\n"],"metadata":{"id":"xeuynfy4Wfxh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 5: Tokenization (Units & Text)\n","from transformers import AutoTokenizer\n","\n","# Tokenizer for HRL Text (e.g., English, using pretrained BPE tokenizer)\n","text_tokenizer = AutoTokenizer.from_pretrained(\"facebook/mbart-large-50\")\n","\n","# Unit tokenizer (integer units, vocab size = n_clusters)\n","class UnitTokenizer:\n","    def __init__(self, vocab_size):\n","        self.vocab_size = vocab_size\n","    def encode(self, seq): return list(map(int, seq.split()))\n","    def decode(self, ids): return \" \".join(map(str, ids))\n","\n","unit_tokenizer = UnitTokenizer(vocab_size=100)\n"],"metadata":{"id":"Hf5GAYaaWkLD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 6: Create PyTorch Dataset\n","MAX_LEN_UNITS = 128\n","MAX_LEN_TEXT = 64\n","\n","class UnitToTextDataset(Dataset):\n","    def __init__(self, unit_seqs, text_seqs):\n","        self.unit_seqs = unit_seqs\n","        self.text_seqs = text_seqs\n","    def __len__(self):\n","        return len(self.unit_seqs)\n","    def __getitem__(self, idx):\n","        u = unit_tokenizer.encode(self.unit_seqs[idx])[:MAX_LEN_UNITS]\n","        t = text_tokenizer.encode(self.text_seqs[idx], truncation=True, max_length=MAX_LEN_TEXT)\n","        return {\n","            \"input_ids\": torch.tensor(u, dtype=torch.long),\n","            \"labels\": torch.tensor(t, dtype=torch.long)\n","        }\n","\n","dataset = UnitToTextDataset(unit_seqs, text_seqs)\n","dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=lambda x: {\n","    \"input_ids\": nn.utils.rnn.pad_sequence([d[\"input_ids\"] for d in x], batch_first=True, padding_value=0),\n","    \"labels\": nn.utils.rnn.pad_sequence([d[\"labels\"] for d in x], batch_first=True, padding_value=-100),\n","})\n"],"metadata":{"id":"N1eaeZYsWntu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 7: Define Transformer Decoder Model\n","class SeqDecoder(nn.Module):\n","    def __init__(self, unit_vocab_size, text_vocab_size, d_model=256, nhead=4, num_layers=4):\n","        super().__init__()\n","        self.unit_emb = nn.Embedding(unit_vocab_size, d_model)\n","        self.text_emb = nn.Embedding(text_vocab_size, d_model)\n","        self.encoder = nn.TransformerEncoder(\n","            nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead),\n","            num_layers=num_layers\n","        )\n","        self.decoder = nn.TransformerDecoder(\n","            nn.TransformerDecoderLayer(d_model=d_model, nhead=nhead),\n","            num_layers=num_layers\n","        )\n","        self.out_proj = nn.Linear(d_model, text_vocab_size)\n","\n","    def forward(self, src, tgt):\n","        src_mask = None\n","        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n","        src = self.unit_emb(src)\n","        tgt = self.text_emb(tgt)\n","        memory = self.encoder(src, src_key_padding_mask=(src == 0))\n","        output = self.decoder(tgt, memory, tgt_mask=tgt_mask)\n","        return self.out_proj(output)\n","\n","model = SeqDecoder(unit_vocab_size=100, text_vocab_size=text_tokenizer.vocab_size).cuda()\n"],"metadata":{"id":"E-Gu3w5hWpJz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 8: Train the Model\n","optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)\n","criterion = nn.CrossEntropyLoss(ignore_index=-100)\n","\n","EPOCHS = 5\n","\n","for epoch in range(EPOCHS):\n","    model.train()\n","    total_loss = 0\n","    for batch in tqdm(dataloader):\n","        input_ids = batch[\"input_ids\"].cuda()\n","        labels = batch[\"labels\"].cuda()\n","        decoder_input = labels[:, :-1]\n","        target = labels[:, 1:]\n","\n","        outputs = model(input_ids, decoder_input)\n","        loss = criterion(outputs.view(-1, outputs.shape[-1]), target.reshape(-1))\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        total_loss += loss.item()\n","\n","    print(f\"Epoch {epoch+1}/{EPOCHS} | Loss: {total_loss/len(dataloader):.4f}\")\n"],"metadata":{"id":"F8iK8ISfWx3l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 9: Save Model\n","model_path = os.path.join(BASE_DIR, 'trained_decoder.pt')\n","torch.save(model.state_dict(), model_path)\n","print(\"Model saved to:\", model_path)\n"],"metadata":{"id":"9cfrNigiW32J"},"execution_count":null,"outputs":[]}]}