{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMEof7+8DM0X56BGSZbTwyS"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8puHkoUKU3nj","executionInfo":{"status":"ok","timestamp":1744852856650,"user_tz":-330,"elapsed":29654,"user":{"displayName":"Shreya Pandey","userId":"06392802961116447132"}},"outputId":"fcf74e9b-5089-44d2-8882-6b91bc9d8348"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["#  Step 1: Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Step 2: Set Working Directory\n","import os\n","BASE_DIR = '/content/drive/MyDrive/speech_understanding_project'\n","DATA_DIR = os.path.join(BASE_DIR, 'data')\n","os.makedirs(DATA_DIR, exist_ok=True)\n"]},{"cell_type":"code","source":["!pip install scikit-learn numpy tqdm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UXlRi79iVTrq","executionInfo":{"status":"ok","timestamp":1744852860072,"user_tz":-330,"elapsed":3412,"user":{"displayName":"Shreya Pandey","userId":"06392802961116447132"}},"outputId":"0443cac4-17aa-427d-b202-0760e9ce7711"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n","Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n"]}]},{"cell_type":"code","source":["import numpy as np\n","from sklearn.cluster import MiniBatchKMeans\n","from tqdm import tqdm\n","import glob"],"metadata":{"id":"1JPz037FVaN-","executionInfo":{"status":"ok","timestamp":1744852865665,"user_tz":-330,"elapsed":3636,"user":{"displayName":"Shreya Pandey","userId":"06392802961116447132"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Step 4: Load Feature Files\n","def load_all_features(feature_dir, max_utts=None, max_frames=1000):\n","    all_feats = []\n","    files = sorted(glob.glob(os.path.join(feature_dir, '*.npy')))\n","    if max_utts: files = files[:max_utts]\n","\n","    for f in tqdm(files):\n","        feat = np.load(f)\n","        if feat.shape[0] > max_frames:\n","            # randomly sample `max_frames` for speed\n","            idx = np.random.choice(feat.shape[0], max_frames, replace=False)\n","            feat = feat[idx]\n","        all_feats.append(feat)\n","\n","    return np.concatenate(all_feats, axis=0)\n","\n","# Load LRL features for clustering (e.g., 10K frames max)\n","lrl_feat_dir = os.path.join(DATA_DIR, 'features_lrl')\n","all_lrl_feats = load_all_features(lrl_feat_dir, max_utts=100, max_frames=100)\n","print(\"All feature shape:\", all_lrl_feats.shape)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"collapsed":true,"id":"t1jhxLhtVdjR","executionInfo":{"status":"error","timestamp":1744852878169,"user_tz":-330,"elapsed":165,"user":{"displayName":"Shreya Pandey","userId":"06392802961116447132"}},"outputId":"78434ef4-94d1-4932-96a0-bf6d91825a03"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"need at least one array to concatenate","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-ff1f8dacf324>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load LRL features for clustering (e.g., 10K frames max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlrl_feat_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features_lrl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mall_lrl_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrl_feat_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_utts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All feature shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_lrl_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-4-ff1f8dacf324>\u001b[0m in \u001b[0;36mload_all_features\u001b[0;34m(feature_dir, max_utts, max_frames)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mall_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Load LRL features for clustering (e.g., 10K frames max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"]}]},{"cell_type":"code","source":["# Train K-Means to Get Pseudo-Units\n","N_CLUSTERS = 100  # ‚Üê number of pseudo-phoneme units\n","\n","kmeans = MiniBatchKMeans(n_clusters=N_CLUSTERS, random_state=42, batch_size=1024)\n","kmeans.fit(all_lrl_feats)\n","\n","# üìÅ Save the model\n","import joblib\n","joblib.dump(kmeans, os.path.join(DATA_DIR, f'kmeans_{N_CLUSTERS}.joblib'))\n","print(f\"KMeans model saved.\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"id":"mPYG7pL6VjNe","executionInfo":{"status":"error","timestamp":1744852905904,"user_tz":-330,"elapsed":45,"user":{"displayName":"Shreya Pandey","userId":"06392802961116447132"}},"outputId":"697c7c78-4811-4500-fd78-f65640776585"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["0it [00:00, ?it/s]\n"]},{"output_type":"error","ename":"ValueError","evalue":"need at least one array to concatenate","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-178a0daac692>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# ‚öôÔ∏è Load LRL features for clustering (e.g., 10K frames max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mlrl_feat_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'features_lrl'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mall_lrl_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_all_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlrl_feat_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_utts\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"All feature shape:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_lrl_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-5-178a0daac692>\u001b[0m in \u001b[0;36mload_all_features\u001b[0;34m(feature_dir, max_utts, max_frames)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mall_feats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# ‚öôÔ∏è Load LRL features for clustering (e.g., 10K frames max)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: need at least one array to concatenate"]}]},{"cell_type":"code","source":["# Quantize Utterances into Pseudo-Units\n","def quantize_features_to_units(feature_dir, kmeans_model, output_path):\n","    files = sorted(glob.glob(os.path.join(feature_dir, '*.npy')))\n","    with open(output_path, 'w') as f:\n","        for file in tqdm(files):\n","            utt_id = os.path.basename(file).replace('.npy', '')\n","            features = np.load(file)\n","            units = kmeans_model.predict(features)\n","            unit_seq = \" \".join(map(str, units))\n","            f.write(f\"{utt_id}\\t{unit_seq}\\n\")\n","    print(f\"Discrete units saved to: {output_path}\")\n"],"metadata":{"id":"F8sra0SBVzHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Step 7: Save Discrete Sequences\n","quantized_dir = os.path.join(DATA_DIR, 'units')\n","os.makedirs(quantized_dir, exist_ok=True)\n","\n","# LRL Pseudo-Units\n","quantize_features_to_units(\n","    feature_dir=lrl_feat_dir,\n","    kmeans_model=kmeans,\n","    output_path=os.path.join(quantized_dir, 'lrl_units.txt')\n",")\n","\n","# HRL Units (optional, for supervised comparison or decoder training)\n","quantize_features_to_units(\n","    feature_dir=os.path.join(DATA_DIR, 'features_hrl'),\n","    kmeans_model=kmeans,\n","    output_path=os.path.join(quantized_dir, 'hrl_units.txt')\n",")\n"],"metadata":{"id":"YsKCu9whV104"},"execution_count":null,"outputs":[]}]}